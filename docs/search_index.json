[["index.html", "Statistics for Economics with R Preface", " Statistics for Economics with R Logan Kelly, Ph.D. 2020-12-01 Preface In this text, the following boxes will be used to set off content. Explinations of sample R code will be placed in boxes like this. "],["linear-regression-part-i.html", "Chapter 1 Linear Regression - Part I 1.1 R Packages Used in this Chapter 1.2 Assumptions of the Classical Linear Regression Model 1.3 Assumption 1. Linearity 1.4 Assumption 2: Mean of the residuals 1.5 Assumption 3: Normally distributed residuals 1.6 Case Study 1: How risky is that stock?", " Chapter 1 Linear Regression - Part I 1.1 R Packages Used in this Chapter Base R has a great deal of functionality, but the real power of R is that thousands of people developing packages that expand the capabilities of R. In his chapter we will be using the following packages. tidyverse The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures (see https://www.tidyverse.org/). psych A general purpose toolbox for personality, psychometric theory and experimental psychology (see https://cran.r-project.org/web/packages/psych/index.html) The following code chunk test weather each package has been installed, installs the package if needed, and then loads the package. if (!require(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;) library(tidyverse) if (!require(&quot;psych&quot;)) install.packages(&quot;psych&quot;) library(psych) install.packages() command installs the package and the library() command loads the package. For now, you can copy this code and paste this code to use it in your own analysis. 1.2 Assumptions of the Classical Linear Regression Model The dependent variable is linearly related to the coefficients of the model and the model is correctly specified. The mean of the error term is zero. The error term is normally distributed. (Not absolutely necessary) No perfect multicollinearity. No independent variable has a perfect linear relationship with any of the other independent variables. The error term has a constant variance. No heteroscedasticity. The error terms are uncorrelated with each other. No autocorrelation or serial correlation. The independent variable(s) is/are uncorrelated with the equation error term. In this chapter, we will focus on the first three assumptions, and we will discuss an important model diagnostic tool, the residual vs. fit plot. 1.3 Assumption 1. Linearity To use linear regression, the relationship we are studying must be linear. Duh, right? But is is a very common mistake to assume linearity without checking. The best way to check is to plot the data using scatter plots. Lets use a famous data set known as Anscombes Quartet as an example of identifying linear and non linear relationships. Anscombes Quartet is a set of four pairs of x/y data that all yield identical regression output. Look at the summary statistics fro Anscombes Quartet. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIyBMb2FkIHBzeWNoIHBhY2thZ2VcbmlmICghcmVxdWlyZShcInBzeWNoXCIpKSBpbnN0YWxsLnBhY2thZ2VzKFwicHN5Y2hcIilcbmxpYnJhcnkocHN5Y2gpXG5cbiMjIExvYWQgYW5kIGRlc2NyaWJlIEFuc2NvbWJlJ3MgUXVhcnRldFxuZGF0YShcImFuc2NvbWJlXCIpXG5kZXNjcmliZShhbnNjb21iZSwgc2tldyA9IEYscmFuZ2VzID0gRikifQ== The data() command loads built in R data sets. R has several built in data sets for learning R. data(\"anscombe\") loads Anscombes Quartet. The describe() command is from the psych package. You will sometimes see this indicated in documentation as follows: psych::describe() it does a nicer job of presenting summary statistics. The arguments skew and ranges are bothe set to false to keep the output simple. Notice that the summary statistics fore each of the xs and for each of the ys are nearly identical. Now lets look at the output from linear regression of the first and second x/ys of the data set. In R, the lm() command is used to estimate linear regression models. The lm stands for linear model. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhbnNjb21iZS5sbTAxIDwtIGxtKHkxIH4geDEsIGRhdGEgPSBhbnNjb21iZSlcbnN1bW1hcnkoYW5zY29tYmUubG0wMSlcbmFuc2NvbWJlLmxtMDIgPC0gbG0oeTIgfiB4MiwgZGF0YSA9IGFuc2NvbWJlKVxuc3VtbWFyeShhbnNjb21iZS5sbTAyKSJ9 lm() is used to fit linear models. The first argument is the formula. the dependent variable is listed first followed by a ~, i.e. a tilde. Then the independent, or predictor, variables are listed. The + (plus) sign is used to separate each independent variable. Note that the output of lm() is not very useful. You need to store the linear model in an object and then use the summary() to display the regression output. Notice that the regression output for the first set of x/ys is nearly identical to the second x/ys, but before we draw any conclusions, lets check that relationships are linear. We will do this by first plotting a scatter plot of x vs y for each pair. Then we will plot the residue vs fit plot. To plot a scatter plot of x vs y for each pair we will use a tool from the tidyverse called ggplot. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImlmICghcmVxdWlyZShcImdncGxvdDJcIikpIGluc3RhbGwucGFja2FnZXMoXCJnZ3Bsb3QyXCIpXG5saWJyYXJ5KGdncGxvdDIpXG5pZiAoIXJlcXVpcmUoXCJkcGx5clwiKSkgaW5zdGFsbC5wYWNrYWdlcyhcImRwbHlyXCIpXG5saWJyYXJ5KGRwbHlyKSIsInNhbXBsZSI6ImFuc2NvbWJlICU+JSBnZ3Bsb3QoYWVzKHg9eDEseT15MSkpICtcbiAgZ2VvbV9wb2ludCgpICtcbiAgZ2d0aXRsZShcIlBsb3QxXCIpXG5cbmFuc2NvbWJlICU+JSBnZ3Bsb3QoYWVzKHg9eDIseT15MikpICtcbiAgZ2VvbV9wb2ludCgpICtcbiAgZ2d0aXRsZShcIlBsb3QyXCIpIn0= To plot a scatter plot of x vs y for each pair we will use a tool from the tidyverse called ggplot. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImlmICghcmVxdWlyZShcImdncGxvdDJcIikpIGluc3RhbGwucGFja2FnZXMoXCJnZ3Bsb3QyXCIpXG5saWJyYXJ5KGdncGxvdDIpXG5pZiAoIXJlcXVpcmUoXCJkcGx5clwiKSkgaW5zdGFsbC5wYWNrYWdlcyhcImRwbHlyXCIpXG5saWJyYXJ5KGRwbHlyKVxuXG5hbnNjb21iZS5sbTAxIDwtIGxtKHkxIH4geDEsIGRhdGEgPSBhbnNjb21iZSlcbmFuc2NvbWJlLmxtMDIgPC0gbG0oeTIgfiB4MiwgZGF0YSA9IGFuc2NvbWJlKSIsInNhbXBsZSI6ImRhdDEgPC0gZGF0YS5mcmFtZShmaXQgPSBmaXR0ZWQoYW5zY29tYmUubG0wMSksXG4gICAgICAgICAgcmVzID0gcmVzaWQoYW5zY29tYmUubG0wMSkpXG5cbmRhdDEgJT4lIGdncGxvdChhZXMoeD1maXQseT1yZXMpKSArXG4gIGdlb21fcG9pbnQoKSArXG4gIGdndGl0bGUoXCJQbG90MVwiKVxuXG5kYXQyIDwtIGRhdGEuZnJhbWUoZml0ID0gZml0dGVkKGFuc2NvbWJlLmxtMDIpLFxucmVzID0gcmVzaWQoYW5zY29tYmUubG0wMikpXG5cbmRhdDIgJT4lIGdncGxvdChhZXMoeD1maXQseT1yZXMpKSArXG4gIGdlb21fcG9pbnQoKSArXG4gIGdndGl0bGUoXCJQbG90MlwiKSJ9 1.4 Assumption 2: Mean of the residuals First exam the summary statistics of the residuals. Both the mean and the median should be very close to zero. Recalling the first of Anscombes Quartet. ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.92127 -0.45577 -0.04136 0.00000 0.70941 1.83882 The mean and median of the residuals for the first of Anscombes Quartet looks good, but lest look at the second of the quartet. ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.9009 -0.7609 0.1291 0.0000 0.9491 1.2691 Notice that the mean is zero, which is good, but the median is larger. While this is not a conclusive model diagnostic, it does trigger a red flag. Lets examine the residuals vs. fit plot. We can see that that in Plot 1 the residuals are randomly spread around zero, but Plot 2 shows obvious pattern. In the case of the the second set of x/y pairs in Anscombes Quartet, the issue is that the relationship is non-linear. Plot 1 Plot 2 We can also use the standard error of regression, or what R calls the Residual standard error. The following function draws a little fancier residual vs. fit plot. It plots shaded regions corresponding to one and two standard errors of regression above and below zero. Most of the residuals should be within one standard error of regression plus or minus zero and nearly all residuals should be within plus or minus two standard error of regression of zero. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImlmICghcmVxdWlyZShcImdncGxvdDJcIikpIGluc3RhbGwucGFja2FnZXMoXCJnZ3Bsb3QyXCIpXG5saWJyYXJ5KGdncGxvdDIpXG5pZiAoIXJlcXVpcmUoXCJkcGx5clwiKSkgaW5zdGFsbC5wYWNrYWdlcyhcImRwbHlyXCIpXG5saWJyYXJ5KGRwbHlyKSIsInNhbXBsZSI6InBsb3QuZml0dGVkIDwtIGZ1bmN0aW9uKGxtb2QpIHtcbiAgZGYgPSBsbW9kJG1vZGVsXG4gIHRpdGxlID0gcGFzdGUoXCJSZXNpZHVhbHMgdnMuIEZpdHMgKERlcGVuZGVudCBWYXJpYWJsZTogXCIsXG4gICAgICAgICAgICAgICAgbmFtZXMoZGYpWzFdLFxuICAgICAgICAgICAgICAgIFwiKVwiLFxuICAgICAgICAgICAgICAgIHNlcCA9IFwiXCIpXG4gIHByIDwtIGRmICU+JSBnZ3Bsb3QoYWVzKHggPSBmaXR0ZWQobG1vZCksIHkgPSByZXNpZChsbW9kKSkpICtcbiAgICBnZW9tX3JpYmJvbihhZXMoeW1pbiA9IC1zaWdtYShsbW9kKSwgeW1heCA9IHNpZ21hKGxtb2QpKSxcbiAgICAgICAgICAgICAgICBmaWxsID0gXCJncmF5XCIsXG4gICAgICAgICAgICAgICAgYWxwaGEgPSAuNSkgK1xuICAgIGdlb21fcmliYm9uKGFlcyh5bWluID0gLXNpZ21hKGxtb2QpICogMiwgeW1heCA9IHNpZ21hKGxtb2QpICogMiksXG4gICAgICAgICAgICAgICAgZmlsbCA9IFwibGlnaHRncmF5XCIsXG4gICAgICAgICAgICAgICAgYWxwaGEgPSAuNSkgK1xuICAgIGdlb21fcG9pbnQoKSArXG4gICAgdGhlbWVfY2xhc3NpYygpICtcbiAgICBnZ3RpdGxlKHRpdGxlKSArXG4gICAgeWxhYihcIlJlc2lkdWxlc1wiKSArXG4gICAgeGxhYihcIkZpdHRlZFwiKSArXG4gICAgZ2VvbV9obGluZSh5aW50ZXJjZXB0ID0gMCxcbiAgICAgICAgICAgICAgIGxpbmV0eXBlID0gXCJzb2xpZFwiLFxuICAgICAgICAgICAgICAgY29sb3IgPSBcImJsYWNrXCIpXG4gIHJldHVybihwcilcbn1cblxuIyBFeGFtcGxlXG5cbmFuc2NvbWJlLmxtMDEgPC0gbG0oeTEgfiB4MSwgZGF0YSA9IGFuc2NvbWJlKVxucGxvdC5maXR0ZWQoYW5zY29tYmUubG0wMSApIn0= 1.5 Assumption 3: Normally distributed residuals This is one of the least important assumptions because linear regression is ver robust with respect to normality of the residuals. A great way to evaluate the normality assumption is to plot a histogram of the residuals. Plot 1 (left column) shows the residuals from the first of the x/y pairs in Anscombes Quartet. I have also plotted a normal cure over the histogram. The histogram is reasonably close to the normal curve. By contrast, Plot 2 (right column) shows the residuals from the second of the x/y pairs in Anscombes Quartet. We have already shown that linear regression is not a good model for this data, and the non normality of the residuals is just one more symptom of that fact. Plot 1 Plot 2 Here is a function that will help plot the histogram of the residuals. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiMgaWYgKCFyZXF1aXJlKFwiZ2dwbG90MlwiKSkgaW5zdGFsbC5wYWNrYWdlcyhcImdncGxvdDJcIilcbiMgbGlicmFyeShnZ3Bsb3QyKVxuIyBpZiAoIXJlcXVpcmUoXCJkcGx5clwiKSkgaW5zdGFsbC5wYWNrYWdlcyhcImRwbHlyXCIpXG4jIGxpYnJhcnkoZHBseXIpIiwic2FtcGxlIjoicGxvdC5ub3JtYWwgPC0gZnVuY3Rpb24obG1vZCkge1xuICByZXMgPC0gcmVzaWQobG1vZClcbiAgaGlzdChyZXMsIHByb2JhYmlsaXR5ID0gVClcbiAgY3VydmUoXG4gICAgZG5vcm0oeCwgbWVhbiA9IG1lYW4ocmVzKSwgc2QgPSAoc2QocmVzKSkpLFxuICAgIGNvbCA9IFwiZGFya2JsdWVcIixcbiAgICBsd2QgPSAyLFxuICAgIGFkZCA9IFRSVUUsXG4gICAgeWF4dCA9IFwiblwiXG4gIClcbn1cblxuIyBFeGFtcGxlXG5cbmFuc2NvbWJlLmxtMDEgPC0gbG0oeTIgfiB4MiwgZGF0YSA9IGFuc2NvbWJlKVxucGxvdC5ub3JtYWwoYW5zY29tYmUubG0wMSApIn0= 1.6 Case Study 1: How risky is that stock? In this case study, we will use linear regression to calculate the beta of a particular stock. What is Beta? Beta is a measure of the volatility of a security or portfolio compared to the market as a whole. Volatility is a common measure of risk, i.e. the more volatile, or variable, the return of an asset are, the more risky the asset is considered to be. How is Beta Calculated? From the Capital Asset Pricing Model (CAPM), we have the following \\[R^e = RF+\\beta(RM-RF)\\] where \\(R\\) is our stock, or portfolio, rate of return, \\(RF\\) is the rate of return on a default risk free asset, and \\(RM\\) is the market rate of return. Define \\(R&#39;\\) to be our stocks excess return, i.e. \\(R-RF\\), and \\(RM&#39;\\) to be the market excess return, i.e. \\(RM-RF\\), then the linear model \\[R = \\alpha + \\beta (RM&#39;) + \\epsilon\\] can be used to estimate Beta. Under the CAPM assumptions, \\(\\alpha = 0\\) and \\(\\beta\\) is the stock, or portfolio, Beta. Enough theory. Lets calculate a Beta. Lets estimate the Beta for Tesla, Inc. (TSLA). To do so we need the following steps. Step 1. Get the data. We need stock price data for Tesla, the market return and the risk free rate of return. Use the S&amp;P 500 Index as the proxy for the market return and 3-month Treasury constant maturity as the risk free rate. The file stock.csv has the excess return for the S&amp;P 500 Tesla and Apple. stock.return &lt;- read.csv(&quot;data/stock.csv&quot;) head(stock.return) ## Date AMZN SP500 TSLA ## 1 11/2/2015 0.3905002 1.1802233 3.26119374 ## 2 11/3/2015 -0.4850048 0.2724079 -2.57751655 ## 3 11/4/2015 2.4703946 -0.3551667 10.59219611 ## 4 11/5/2015 2.2675403 -0.1133064 0.06039556 ## 5 11/6/2015 0.5657680 -0.0347682 0.25423925 ## 6 11/9/2015 -0.5902068 -0.9871565 -3.07221284 Step 2. Estimate the model. We will first plot Teslas excess returns vs. the S&amp;P 500. Then we estimate the linear regression. beta.lm &lt;- lm(TSLA ~ SP500, data = stock.return) summary(beta.lm) ## ## Call: ## lm(formula = TSLA ~ SP500, data = stock.return) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.417 -1.653 -0.105 1.469 17.073 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.15780 0.09325 1.692 0.0909 . ## SP500 1.26545 0.07835 16.152 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.255 on 1218 degrees of freedom ## Multiple R-squared: 0.1764, Adjusted R-squared: 0.1757 ## F-statistic: 260.9 on 1 and 1218 DF, p-value: &lt; 2.2e-16 Step 2. Check the model. We need to check that the model does not violate the assumptions of linear regression. First we check for a linear relationship by plotting Teslas excess returns vs. the S&amp;P 500. stock.return %&gt;% ggplot(aes(x=SP500,y=TSLA)) + geom_point() Next, we check the residuals vs. fit plot. We will use the plot.fitted() function we defined earlier. To use the function in your analysis, simply copy and past it into you R script. plot.fitted(beta.lm) Finally, we check that the residuals are reasonably close to being normally distributed. We will use the plot.normal() function we defined earlier. plot.normal(beta.lm) "],["linear-regression-part-ii.html", "Chapter 2 Linear Regression - Part II 2.1 R Packages Used in this Chapter 2.2 Assumptions of the Classical Linear Regression Model 2.3 Assumption 4: Multicollinearity 2.4 Testing for joint significance", " Chapter 2 Linear Regression - Part II 2.1 R Packages Used in this Chapter Base R has a great deal of functionality, but the real power of R is that thousands of people developing packages that expand the capabilities of R. In his chapter we will be using the following packages. tidyverse The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures (see https://www.tidyverse.org/). psych A general purpose toolbox for personality, psychometric theory and experimental psychology (see https://cran.r-project.org/package=psych) jtools A collection of tools to more efficiently understand and share the results of regression analyses (see https://cran.r-project.org/package=jtools) car Functions to accompany J. Fox and S. Weisberg, An R Companion to Applied Regression, Third Edition, Sage, 2019. (see https://cran.r-project.org/package=car) The following code chunk test weather each package has been installed, installs the package if needed, and then loads the package. if (!require(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;) library(tidyverse) if (!require(&quot;psych&quot;)) install.packages(&quot;psych&quot;) library(psych) if (!require(&quot;jtools&quot;)) install.packages(&quot;jtools&quot;) library(jtools) if (!require(&quot;car&quot;)) install.packages(&quot;car&quot;) library(car) install.packages() command installs the package and the library() command loads the package. For now, you can copy this code and paste this code to use it in your own analysis. 2.2 Assumptions of the Classical Linear Regression Model The dependent variable is linearly related to the coefficients of the model and the model is correctly specified. The mean of the error term is zero. The error term is normally distributed. (Not absolutely necessary) No perfect multicollinearity. No independent variable has a perfect linear relationship with any of the other independent variables. The error term has a constant variance. No heteroscedasticity. The error terms are uncorrelated with each other. No autocorrelation or serial correlation. The independent variable(s) is/are uncorrelated with the equation error term. In this chapter, we will focus on the assumptions 4, 5 and 6. 2.3 Assumption 4: Multicollinearity Multicollinearity is the term used to describe the correlation among the independent variables. Recall that the correlation between variables is a measure of how closely the variables move together. Variables that are positively correlated move in the same direction and variables that are negatively correlated move in opposite directions. Multicollinearity can be a problem when this correlation is high. When there is high correlation among the independent variables, it can be difficult to distinguish between the effects of each independent variable on the dependent variable. To understand this, think of the independent variables as strings pulling on the dependent variable. If the the independent variables pull in different directions, i.e. are uncorrelated with each other, then we can see the effect of each independent variable. But if the independent variables pull in the same direction, i.e. are correlated with each other, then the effects of each independent variable becomes muddled. Multicollinearity can lead to the following problems: Independent variables are incorrectly found to be insignificant. Coefficients have incorrect signs. Perfect multicolinearity occurs when independent variables have a correlation coefficient of 1 or -1. In this case, the perfect collinear variables must be removed to calculate a linear regression. 2.3.1 The Hot Dog Case The Dubuque Hot Dog company produces low price hot dogs. Their competition includes Ball Park, the leading brand, and Oscar Mayer. Ball Park produces two types of hot dogs, regular and all-beef, and is planning on reducing the price its hot dogs. The CEO of Debuque would like impact on Dubuques market share. Here are the current and new prices. Hot Dog Retail Pricing Company Product Current Price New Price Dubuque Regular 1.49 Oscar Mayer Regular 1.69 Ball Park Regular 1.79 1.45 Ball Park All-beef 1.89 1.55 The CEO believes that the impact will be small because Oscar Mayer is Dubuques leading competitor, and presents you with the following regression analysis to support his argument. ## MODEL INFO: ## Observations: 113 ## Dependent Variable: MKTDUB ## Type: OLS linear regression ## ## MODEL FIT: ## F(4,108) = 29.97881, p = 0.00000 ## R² = 0.52614 ## Adj. R² = 0.50859 ## ## Standard errors: OLS ## ---------------------------------------------------------------------- ## Est. S.E. t val. p VIF ## ----------------- ---------- --------- ---------- --------- ---------- ## (Intercept) 0.04046 0.01418 2.85349 0.00518 ## pdub -0.00076 0.00008 -9.46040 0.00000 1.34784 ## poscar 0.00026 0.00008 3.12607 0.00228 1.65545 ## pbpreg 0.00039 0.00025 1.57790 0.11751 14.69948 ## pbpbeef 0.00006 0.00021 0.27832 0.78130 13.90586 ## ---------------------------------------------------------------------- The CEO notes that the Ball Park hot dog prices, pbpreg and pbpbeef, have an insignificant effect on Dubuques market share. We are given the data used to estimate the regression. hotdog &lt;- read.csv(&quot;data/Hotdog.csv&quot;) head(hotdog) ## MKTDUB pdub poscar pbpreg pbpbeef ## 1 0.0454565 149 169 169 180 ## 2 0.0930145 149 199 189 205 ## 3 0.0596656 189 199 189 205 ## 4 0.0345966 189 199 189 201 ## 5 0.0276536 189 169 159 166 ## 6 0.0294224 189 199 189 196 Deffinitions of each variables: MKTDUB: Dubuques market share pdub: Dubuques hot dog price poscar: Oscar Mayers hot dog price pbpreg: Ball Parks regular hot dog price pbpbeef: Ball Parks all-beef hot dog price 2.3.2 Checking for multicollinearity The variance inflation factor is an indicator of a multicollinearity problem. You can calculate the variance inflation factor by adding vif=T to the summary() command. A VIF greater than 10 may indicate a problem with multicollinearity. This is not my favorite way of checking though. A better way is to plot a Paris Panel you can do this the pairs.panels() command from the psych package. psych::pairs.panels(hotdog) The Paris Panel plot provides skater plots of each pair of variable in the data frame, a histogram of each variable and th correlation between each pair of variables in the data frame. If the absolute value of the correlation coefficient is over 0.7, you need to watch for Multicollinearity. If the absolute value of the correlation coefficient is over 0.9, you Multicollinearity is a problem, and you should consider dropping one of the variables from your model. 2.4 Testing for joint significance We could drop one of the Ball Park Hot dog prices from our regression model as follows. Note that we use summ() from the jtools package to summarize the model. ## MODEL INFO: ## Observations: 113 ## Dependent Variable: MKTDUB ## Type: OLS linear regression ## ## MODEL FIT: ## F(3,109) = 40.28690, p = 0.00000 ## R² = 0.52580 ## Adj. R² = 0.51275 ## ## Standard errors: OLS ## --------------------------------------------------------------------- ## Est. S.E. t val. p VIF ## ----------------- ---------- --------- ---------- --------- --------- ## (Intercept) 0.04007 0.01405 2.85198 0.00520 ## pdub -0.00076 0.00008 -9.60418 0.00000 1.32830 ## poscar 0.00026 0.00008 3.13979 0.00218 1.65545 ## pbpreg 0.00046 0.00008 5.88048 0.00000 1.45475 ## --------------------------------------------------------------------- In this model, the price of Ball Park regular hot dogs is significant. However, the CEO in unconvinced. He is concerned that leaving price of Ball Park all-beef hot dogs invalidates the model. We explain the problem of multicollinarity, but the CEO remains skeptical. In this situation, an F-test of joint significance is the perfect tool. The F-test of joint hypothesis test looks like this \\[\\begin{array}{l} {H_0}:{\\beta _{pbpreg}} = {\\beta _{pbpbeef}} = 0\\\\ {H_a}:{\\beta _{pbpreg}} \\ne 0\\; \\vee \\;{\\beta _{pbpbeef}} \\ne 0 \\end{array}\\] This F-test asks: Are they jointly significant? The test can be preformed using the linearHypothesis() comand from the car package. car::linearHypothesis(hotdog.lm01, c(&quot;pbpreg&quot;,&quot;pbpbeef&quot;)) ## Linear hypothesis test ## ## Hypothesis: ## pbpreg = 0 ## pbpbeef = 0 ## ## Model 1: restricted model ## Model 2: MKTDUB ~ pdub + poscar + pbpreg + pbpbeef ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 110 0.014258 ## 2 108 0.010816 2 0.0034416 17.182 3.32e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Since the F-statistic is large (the p-value is small). We reject the null hypothesis and conclude that Ball Park prices are jointly significant. "]]
