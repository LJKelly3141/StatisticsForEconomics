[["index.html", "Statistics for Economics with R Preface", " Statistics for Economics with R Logan Kelly, Ph.D. 2020-11-20 Preface In this text, the following boxes will be used to set off content. Explinations of sample R code will be placed in boxes like this. "],["linear-regression-part-i.html", "Chapter 1 Linear Regression - Part I 1.1 R Packages Used in this Chapter 1.2 Assumptions of the Classical Linear Regression Model 1.3 Assumption 1. Linearity 1.4 Asumption 2: Mean of the residules. 1.5 Asumption 2: Normally distributed residuals", " Chapter 1 Linear Regression - Part I 1.1 R Packages Used in this Chapter Base R has a great deal of functionality, but the real power of R is that thousands of people developing packages that expand the capabilities of R. In his chapter we will be using the following packages. tidyverse The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures (see https://www.tidyverse.org/). Rmisc The Rmisc library contains many functions useful for data analysis and utility operations (see https://cran.r-project.org/web/packages/Rmisc/index.html). psych A general purpose toolbox for personality, psychometric theory and experimental psychology (see https://cran.r-project.org/web/packages/psych/index.html) The following code chunk test weather each package has been installed, installs the package if needed, and then loads the package. if (!require(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;) library(tidyverse) if (!require(&quot;Rmisc&quot;)) install.packages(&quot;Rmisc&quot;) library(Rmisc) if (!require(&quot;psych&quot;)) install.packages(&quot;psych&quot;) library(psych) install.packages() command installs the package and the library() command loads the package. For now, you can copy this code and paste this code to use it in your own analysis. 1.2 Assumptions of the Classical Linear Regression Model The dependent variable is linearly related to the coefficients of the model and the model is correctly specified. The mean of the error term is zero. The error term is normally distributed. (Not absolutely necessary) No perfect multicollinearity. No independent variable has a perfect linear relationship with any of the other independent variables. The error term has a constant variance. No heteroscedasticity. The error terms are uncorrelated with each other. No autocorrelation or serial correlation. The independent variable(s) is/are uncorrelated with the equation error term. In this chapter, we will focus on the first three assumptions, and we will discuss an important model diagnostic tool, the residual vs. fit plot. 1.3 Assumption 1. Linearity To use linear regression, the relationship we are studying must be linear. Duh, right? But is is a very common mistake to assume linearity without checking. The best way to check is to plot the data using scatter plots. Lets use a famous data set known as Anscombes Quartet as an example of identifying linear and non linear relationships. Anscombes Quartet is a set of four pairs of x/y data that all yield identical regression output. Look at the summary statistics fro Anscombes Quartet. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIyBMb2FkIHBzeWNoIHBhY2thZ2VcbmlmICghcmVxdWlyZShcInBzeWNoXCIpKSBpbnN0YWxsLnBhY2thZ2VzKFwicHN5Y2hcIilcbmxpYnJhcnkocHN5Y2gpXG5cbiMjIExvYWQgYW5kIGRlc2NyaWJlIEFuc2NvbWJlJ3MgUXVhcnRldFxuZGF0YShcImFuc2NvbWJlXCIpXG5kZXNjcmliZShhbnNjb21iZSwgc2tldyA9IEYscmFuZ2VzID0gRikifQ== The data() command loads built in R data sets. R has several built in data sets for learning R. data(\"anscombe\") loads Anscombes Quartet. The describe() command is from the psych package. You will sometimes see this indicated in documentation as follows: psych::describe() it does a nicer job of presenting summary statistics. The arguments skew and ranges are bothe set to false to keep the output simple. Notice that the summary statistics fore each of the xs and for each of the ys are nearly identical. Now lets look at the output from linear regression of the first and second x/ys of the data set. In R, the lm() command is used to estimate linear regression models. The lm stands for linear model. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhbnNjb21iZS5sbTAxIDwtIGxtKHkxIH4geDEsIGRhdGEgPSBhbnNjb21iZSlcbnN1bW1hcnkoYW5zY29tYmUubG0wMSlcbmFuc2NvbWJlLmxtMDIgPC0gbG0oeTIgfiB4MiwgZGF0YSA9IGFuc2NvbWJlKVxuc3VtbWFyeShhbnNjb21iZS5sbTAyKSJ9 lm() is used to fit linear models. The first argument is the formula. the dependent variable is listed first followed by a ~, i.e. a tilde. Then the independent, or predictor, variables are listed. The + (plus) sign is used to separate each independent variable. Note that the output of lm() is not very useful. You need to store the linear model in an object and then use the summary() to display the regression output. Notice that the regression output for the first set of x/ys is nearly identical to the second x/ys, but before we draw any conclusions, lets check that relationships are linear. We will do this by first plotting a scatter plot of x vs y for each pair. Then we will plot the residue vs fit plot. To plot a scatter plot of x vs y for each pair we will use a tool from the tidyverse called ggplot. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImlmICghcmVxdWlyZShcImdncGxvdDJcIikpIGluc3RhbGwucGFja2FnZXMoXCJnZ3Bsb3QyXCIpXG5saWJyYXJ5KGdncGxvdDIpXG5pZiAoIXJlcXVpcmUoXCJkcGx5clwiKSkgaW5zdGFsbC5wYWNrYWdlcyhcImRwbHlyXCIpXG5saWJyYXJ5KGRwbHlyKSIsInNhbXBsZSI6ImFuc2NvbWJlICU+JSBnZ3Bsb3QoYWVzKHg9eDEseT15MSkpICtcbiAgZ2VvbV9wb2ludCgpICtcbiAgZ2d0aXRsZShcIlBsb3QxXCIpXG5cbmFuc2NvbWJlICU+JSBnZ3Bsb3QoYWVzKHg9eDIseT15MikpICtcbiAgZ2VvbV9wb2ludCgpICtcbiAgZ2d0aXRsZShcIlBsb3QyXCIpIn0= To plot a scatter plot of x vs y for each pair we will use a tool from the tidyverse called ggplot. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImlmICghcmVxdWlyZShcImdncGxvdDJcIikpIGluc3RhbGwucGFja2FnZXMoXCJnZ3Bsb3QyXCIpXG5saWJyYXJ5KGdncGxvdDIpXG5pZiAoIXJlcXVpcmUoXCJkcGx5clwiKSkgaW5zdGFsbC5wYWNrYWdlcyhcImRwbHlyXCIpXG5saWJyYXJ5KGRwbHlyKVxuXG5hbnNjb21iZS5sbTAxIDwtIGxtKHkxIH4geDEsIGRhdGEgPSBhbnNjb21iZSlcbmFuc2NvbWJlLmxtMDIgPC0gbG0oeTIgfiB4MiwgZGF0YSA9IGFuc2NvbWJlKSIsInNhbXBsZSI6ImRhdDEgPC0gZGF0YS5mcmFtZShmaXQgPSBmaXR0ZWQoYW5zY29tYmUubG0wMSksXG4gICAgICAgICAgcmVzID0gcmVzaWQoYW5zY29tYmUubG0wMSkpXG5cbmRhdDEgJT4lIGdncGxvdChhZXMoeD1maXQseT1yZXMpKSArXG4gIGdlb21fcG9pbnQoKSArXG4gIGdndGl0bGUoXCJQbG90MVwiKVxuXG5kYXQyIDwtIGRhdGEuZnJhbWUoZml0ID0gZml0dGVkKGFuc2NvbWJlLmxtMDIpLFxucmVzID0gcmVzaWQoYW5zY29tYmUubG0wMikpXG5cbmRhdDIgJT4lIGdncGxvdChhZXMoeD1maXQseT1yZXMpKSArXG4gIGdlb21fcG9pbnQoKSArXG4gIGdndGl0bGUoXCJQbG90MlwiKSJ9 1.4 Asumption 2: Mean of the residules. First exam the summary statistics of the residuals. Both the mean and the median should be very close to zero. Recalling the first of Anscombes Quartet. ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.92127 -0.45577 -0.04136 0.00000 0.70941 1.83882 The mean and median of the residuals for the first of Anscombes Quartet looks good, but lest look at the second of the quartet. ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.9009 -0.7609 0.1291 0.0000 0.9491 1.2691 Notice that the mean is zero, which is good, but the median is larger. While this is not a conclusive model diagnostic, it does trigger a red flag. Lets examine the residuals vs. fit plot. We can see that that in Plot 1 the residuals are randomly spread around zero, but Plot 2 shows obvious pattern. In the case of the the second set of x/y pairs in Anscombes Quartet, the issue is that the relationship is non-linear. Plot 1 Plot 2 We can also use the standard error of regression, or what R calls the Residual standard error. The following function draws a little fancier residual vs. fit plot. It plots shaded regions corresponding to one and two standard errors of regression above and below zero. Most of the residuals should be within one standard error of regression plus or minus zero and nearly all residuals should be within plus or minus two standard error of regression of zero. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6ImlmICghcmVxdWlyZShcImdncGxvdDJcIikpIGluc3RhbGwucGFja2FnZXMoXCJnZ3Bsb3QyXCIpXG5saWJyYXJ5KGdncGxvdDIpXG5pZiAoIXJlcXVpcmUoXCJkcGx5clwiKSkgaW5zdGFsbC5wYWNrYWdlcyhcImRwbHlyXCIpXG5saWJyYXJ5KGRwbHlyKSIsInNhbXBsZSI6InBsb3QuZml0dGVkIDwtIGZ1bmN0aW9uKGxtb2QpIHtcbiAgZGYgPSBsbW9kJG1vZGVsXG4gIHRpdGxlID0gcGFzdGUoXCJSZXNpZHVhbHMgdnMuIEZpdHMgKERlcGVuZGVudCBWYXJpYWJsZTogXCIsXG4gICAgICAgICAgICAgICAgbmFtZXMoZGYpWzFdLFxuICAgICAgICAgICAgICAgIFwiKVwiLFxuICAgICAgICAgICAgICAgIHNlcCA9IFwiXCIpXG4gIHByIDwtIGRmICU+JSBnZ3Bsb3QoYWVzKHggPSBmaXR0ZWQobG1vZCksIHkgPSByZXNpZChsbW9kKSkpICtcbiAgICBnZW9tX3JpYmJvbihhZXMoeW1pbiA9IC1zaWdtYShsbW9kKSwgeW1heCA9IHNpZ21hKGxtb2QpKSxcbiAgICAgICAgICAgICAgICBmaWxsID0gXCJncmF5XCIsXG4gICAgICAgICAgICAgICAgYWxwaGEgPSAuNSkgK1xuICAgIGdlb21fcmliYm9uKGFlcyh5bWluID0gLXNpZ21hKGxtb2QpICogMiwgeW1heCA9IHNpZ21hKGxtb2QpICogMiksXG4gICAgICAgICAgICAgICAgZmlsbCA9IFwibGlnaHRncmF5XCIsXG4gICAgICAgICAgICAgICAgYWxwaGEgPSAuNSkgK1xuICAgIGdlb21fcG9pbnQoKSArXG4gICAgdGhlbWVfY2xhc3NpYygpICtcbiAgICBnZ3RpdGxlKHRpdGxlKSArXG4gICAgeWxhYihcIlJlc2lkdWxlc1wiKSArXG4gICAgeGxhYihcIkZpdHRlZFwiKSArXG4gICAgZ2VvbV9obGluZSh5aW50ZXJjZXB0ID0gMCxcbiAgICAgICAgICAgICAgIGxpbmV0eXBlID0gXCJzb2xpZFwiLFxuICAgICAgICAgICAgICAgY29sb3IgPSBcImJsYWNrXCIpXG4gIHJldHVybihwcilcbn1cblxuIyBFeGFtcGxlXG5cbmFuc2NvbWJlLmxtMDEgPC0gbG0oeTEgfiB4MSwgZGF0YSA9IGFuc2NvbWJlKVxucGxvdC5maXR0ZWQoYW5zY29tYmUubG0wMSApIn0= 1.5 Asumption 2: Normally distributed residuals Plot 1 Plot 2 Here is a function that will help eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6IiMgaWYgKCFyZXF1aXJlKFwiZ2dwbG90MlwiKSkgaW5zdGFsbC5wYWNrYWdlcyhcImdncGxvdDJcIilcbiMgbGlicmFyeShnZ3Bsb3QyKVxuIyBpZiAoIXJlcXVpcmUoXCJkcGx5clwiKSkgaW5zdGFsbC5wYWNrYWdlcyhcImRwbHlyXCIpXG4jIGxpYnJhcnkoZHBseXIpIiwic2FtcGxlIjoicGxvdC5ub3JtYWwgPC0gZnVuY3Rpb24obG1vZCkge1xuICByZXMgPC0gcmVzaWQobG1vZClcbiAgaGlzdChyZXMsIHByb2JhYmlsaXR5ID0gVClcbiAgY3VydmUoXG4gICAgZG5vcm0oeCwgbWVhbiA9IG1lYW4ocmVzKSwgc2QgPSAoc2QocmVzKSkpLFxuICAgIGNvbCA9IFwiZGFya2JsdWVcIixcbiAgICBsd2QgPSAyLFxuICAgIGFkZCA9IFRSVUUsXG4gICAgeWF4dCA9IFwiblwiXG4gIClcbn1cblxuIyBFeGFtcGxlXG5cbmFuc2NvbWJlLmxtMDEgPC0gbG0oeTIgfiB4MiwgZGF0YSA9IGFuc2NvbWJlKVxucGxvdC5ub3JtYWwoYW5zY29tYmUubG0wMSApIn0= "],["dealing-with-catagorical-data.html", "Chapter 2 Dealing with Catagorical Data", " Chapter 2 Dealing with Catagorical Data "],["dealing-with-heteroscdaticity-and-nonlinearity.html", "Chapter 3 Dealing with Heteroscdaticity and Nonlinearity", " Chapter 3 Dealing with Heteroscdaticity and Nonlinearity "]]
